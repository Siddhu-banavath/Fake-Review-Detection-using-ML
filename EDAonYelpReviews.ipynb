{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Exploratory Data Analysis**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train = pd.read_csv('../input/yelp-train/train.csv',header=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.columns = ['deceptive','text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test = pd.read_csv('../input/yelptest/test.csv',header=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test.columns = ['deceptive','text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_test.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(data_train.deceptive)\nplt.xlabel('Deceptive')\nplt.title('Number of Deceptive and Non Deceptive reviews (Deceptive=1 & NonDeceptive=2)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#dataset description\ndata_train.groupby('deceptive').describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#word count\ndata_train['word_count'] = data_train['text'].apply(lambda x: len(str(x).split(\" \")))\ndata_train[['text','word_count']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#character count including spaces\ndata_train['char_count'] = data_train['text'].str.len() ## this also includes spaces\ndata_train[['text','char_count']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#average word length\ndef avg_word(sentence):\n  words = sentence.split()\n  return (sum(len(word) for word in words)/len(words))\n\ndata_train['avg_word'] = data_train['text'].apply(lambda x: avg_word(x))\ndata_train[['text','avg_word']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#no of stopwords\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\n\ndata_train['stopwords'] = data_train['text'].apply(lambda x: len([x for x in x.split() if x in stop]))\ndata_train[['text','stopwords']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#no of special characters\ndata_train['spchar'] = data_train['text'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\ndata_train[['text','spchar']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#no of numerics\ndata_train['numerics'] = data_train['text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\ndata_train[['text','numerics']].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#no of uppercase characters\ndata_train['upper'] = data_train['text'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\ndata_train[['text','upper']].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preprocessing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#to lowercase\ndata_train['text'] = data_train['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ndata_train['text'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing punctuation\ndata_train['text'] = data_train['text'].str.replace('[^\\w\\s]','')\ndata_train['text'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing stop words\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\ndata_train['text'] = data_train['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\ndata_train['text'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing common word\nfreq = pd.Series(' '.join(data_train['text']).split()).value_counts()[:10]\nfreq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing common word\nfreq = list(freq.index)\ndata_train['text'] = data_train['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\ndata_train['text'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#remvoing rare words\nfreq = pd.Series(' '.join(data_train['text']).split()).value_counts()[-10:]\nfreq","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#removing rare words\nfreq = list(freq.index)\ndata_train['text'] = data_train['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\ndata_train['text'].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#spelling correction\nfrom textblob import TextBlob\ndata_train['text'][:5].apply(lambda x: str(TextBlob(x).correct()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tokenization\nTextBlob(data_train['text'][1]).words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#stemming\nfrom nltk.stem import PorterStemmer\nst = PorterStemmer()\ndata_train['text'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lemmetization\nfrom textblob import Word\ndata_train['text'] = data_train['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\ndata_train['text'].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Advance Text Processing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#N-grams\nTextBlob(data_train['text'][0]).ngrams(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Term frequency\ntf1 = (data_train['text'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\ntf1.columns = ['words','tf']\ntf1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#inverse document frequency\nfor i,word in enumerate(tf1['words']):\n  tf1.loc[i, 'idf'] = np.log(data_train.shape[0]/(len(data_train[data_train['text'].str.contains(word)])))\n\ntf1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#term freq - inverse document freq\ntf1['tfidf'] = tf1['tf'] * tf1['idf']\ntf1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sparse matrix tf-idf freq\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n stop_words= 'english',ngram_range=(1,1))\ntrain_vect = tfidf.fit_transform(data_train['text'])\n\ntrain_vect","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Bag of Words\nfrom sklearn.feature_extraction.text import CountVectorizer\nbow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\ntrain_bow = bow.fit_transform(data_train['text'])\ntrain_bow","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = data_train['text'].astype(str)\ny = data_train['deceptive']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x, y,\n                                                    stratify=y, \n                                                    test_size=0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_words = 1000\nmax_len = 150\ntok = Tokenizer(num_words=max_words)\ntok.fit_on_texts(x_train)\nsequences = tok.texts_to_sequences(x_train)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_words = 1000\nmax_len = 150\ntok = Tokenizer(num_words=max_words)\ntok.fit_on_texts(x_test)\nsequences_test = tok.texts_to_sequences(x_test)\nsequences_matrix_test = sequence.pad_sequences(sequences_test,maxlen=max_len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=None,lower=True,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',split=' ',char_level=False)\ntokenizer.fit_on_texts(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer.fit_on_texts(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train1 = tokenizer.texts_to_sequences(x_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test1=tokenizer.texts_to_sequences(x_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_index = tokenizer.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size = len(word_index)\nprint('Vocab size: {}'.format(vocab_size))\nlongest = max(len(seq) for seq in x_train)\nprint(\"Longest comment size: {}\".format(longest))\naverage = np.mean([len(seq) for seq in x_train])\nprint(\"Average comment size: {}\".format(average))\nstdev = np.std([len(seq) for seq in x_train])\nprint(\"Stdev of comment size: {}\".format(stdev))\nmax_len = int(average + stdev * 3)\nprint('Max comment size: {}'.format(max_len))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"processed_x_train = pad_sequences(x_train1, maxlen=max_len, padding='post', truncating='post')\nprocessed_x_test = pad_sequences(x_test1, maxlen=max_len, padding='post', truncating='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.optimizers import RMSprop,Nadam\nfrom keras.callbacks import EarlyStopping","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def RNN():\n    inputs = Input(name='inputs',shape=[max_len])\n    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n    layer = LSTM(64)(layer)\n    layer = Dense(256,name='FC1')(layer)\n    layer = Activation('relu')(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(1,name='out_layer')(layer)\n    layer = Activation('sigmoid')(layer)\n    model = Model(inputs=inputs,outputs=layer)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RNN()\nmodel.summary()\nmodel.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(processed_x_train,y_train,batch_size=128,epochs=10,\n          validation_data=(processed_x_test,y_test),callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras.backend\nfrom keras.models import Sequential, load_model\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers import Dense\nfrom keras.layers import Flatten","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import CuDNNGRU, Dense, Conv1D, MaxPooling1D\nfrom keras.layers import Dropout, GlobalMaxPooling1D, BatchNormalization, LSTM\nfrom keras.layers import Bidirectional","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Embeddings ------- GloVe 100D ------"},{"metadata":{"trusted":true},"cell_type":"code","source":"embeddings_index = {}\nf = open(os.path.join('../input/glove-global-vectors-for-word-representation', 'glove.6B.100d.txt'))\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('Found %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"embedding_dim = 100\nk = 0\nembedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # Words not found in embedding index will be all-zeros.\n        k += 1\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## create model\nmodel_glove = Sequential()\nmodel_glove.add(Embedding(vocab_size + 1, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=True))\nmodel_glove.add(Dropout(0.2))\nmodel_glove.add(Conv1D(64, 5, activation='relu'))\nmodel_glove.add(MaxPooling1D(pool_size=4))\nmodel_glove.add(LSTM(100))\nmodel_glove.add(Dense(1, activation='sigmoid'))\n#model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_glove.summary()\nmodel_glove.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_glove.fit(processed_x_train,y_train,batch_size=128,epochs=10,\n          validation_data=(processed_x_test,y_test),callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"LSTM Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initate model\nmodel3 = Sequential()\n\n# Add Embedding layer\nmodel3.add(Embedding(vocab_size + 1, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=True))\n\n# Add Recurrent layer\n#model.add(Bidirectional(CuDNNGRU(300, return_sequences=True)))\nmodel3.add(LSTM(60, return_sequences=True, name='lstm_layer'))\nmodel3.add(LSTM(30, return_sequences=True, name='lstm_layer2'))\nmodel3.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\nmodel3.add(MaxPooling1D(3))\nmodel3.add(GlobalMaxPooling1D())\nmodel3.add(BatchNormalization())\n\n# Add fully connected layers\nmodel3.add(Dense(50, activation='relu'))\nmodel3.add(Dropout(0.3))\nmodel3.add(Dense(1, activation='sigmoid'))\n\n# Summarize the model\nmodel3.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CNN GloVe Model 2**\n\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32') <br>\nembedded_sequences = embedding_layer(sequence_input)<br>\nl_cov1= Conv1D(128, 5, activation='relu')(embedded_sequences)<br>\nl_pool1 = MaxPooling1D(5)(l_cov1)<br>\nl_cov2 = Conv1D(128, 5, activation='relu')(l_pool1)<br>\nl_pool2 = MaxPooling1D(5)(l_cov2)<br>\nl_cov3 = Conv1D(128, 5, activation='relu')(l_pool2)<br>\nl_pool3 = MaxPooling1D(35)(l_cov3)  # global max pooling<br>\nl_flat = Flatten()(l_pool3)<br>\nl_dense = Dense(128, activation='relu')(l_flat)<br>\npreds = Dense(2, activation='softmax')<br>"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}